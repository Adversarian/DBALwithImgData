{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from scipy import stats\n",
    "from modAL.models import ActiveLearner\n",
    "from skorch import NeuralNetClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class: Convolutional Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_filters: int = 32,\n",
    "        kernel_size: int = 4,\n",
    "        dense_layer: int = 128,\n",
    "        img_rows: int = 28,\n",
    "        img_cols: int = 28,\n",
    "        maxpool: int = 2,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Basic Architecture of CNN proposed in the paper, I have modified softmax\n",
    "        activation function to logsoftmax to penalise large error and improve training\n",
    "        efficiency.\n",
    "\n",
    "        Attributes:\n",
    "            num_filters: Number of filters, out channel for 1st and 2nd conv layers,\n",
    "            kernel_size: Kernel size of convolution,\n",
    "            dense_layer: Dense layer units,\n",
    "            img_rows: Height of input image,\n",
    "            img_cols: Width of input image,\n",
    "            maxpool: Max pooling size\n",
    "        \"\"\"\n",
    "        super(ConvNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, num_filters, kernel_size, 1)\n",
    "        self.conv2 = nn.Conv2d(num_filters, num_filters, kernel_size, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(\n",
    "            num_filters\n",
    "            * ((img_rows - 2 * kernel_size + 2) // 2)\n",
    "            * ((img_cols - 2 * kernel_size + 2) // 2),\n",
    "            dense_layer,\n",
    "        )\n",
    "        self.fc2 = nn.Linear(dense_layer, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        out = F.log_softmax(x, dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class: LoadData - Download and Split Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadData:\n",
    "    \"\"\"Download, split and shuffle dataset into train, validate, test and pool\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.mnist_train, self.mnist_test = self.download_dataset()\n",
    "        (\n",
    "            self.X_train_All,\n",
    "            self.y_train_All,\n",
    "            self.X_val,\n",
    "            self.y_val,\n",
    "            self.X_pool,\n",
    "            self.y_pool,\n",
    "            self.X_test,\n",
    "            self.y_test,\n",
    "        ) = self.split_and_load_dataset()\n",
    "        self.X_init, self.y_init = self.preprocess_training_data()\n",
    "\n",
    "    def tensor_to_np(self, tensor_data: torch.Tensor) -> np.ndarray:\n",
    "        \"\"\"Since Skorch doesn not support dtype of torch.Tensor, we will modify\n",
    "        the dtype to numpy.ndarray\n",
    "\n",
    "        Attribute:\n",
    "            tensor_data: Data of class type=torch.Tensor\n",
    "        \"\"\"\n",
    "        np_data = tensor_data.detach().numpy()\n",
    "        return np_data\n",
    "\n",
    "\n",
    "    def check_MNIST_folder(self) -> bool:\n",
    "        \"\"\"Check whether MNIST folder exists, if yes remove and redownload.\"\"\"\n",
    "        if os.path.isfile(\"MNIST/\"):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def download_dataset(self):\n",
    "        \"\"\"Load MNIST dataset for training and test set.\"\"\"\n",
    "        transform = transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize(0.5, 0.5)]\n",
    "        )\n",
    "        download = self.check_MNIST_folder()\n",
    "        mnist_train = MNIST(\".\", train=True, download=download, transform=transform)\n",
    "        mnist_test = MNIST(\".\", train=False, download=download, transform=transform)\n",
    "        return mnist_train, mnist_test\n",
    "\n",
    "    def split_and_load_dataset(\n",
    "        self,\n",
    "        train_size: int = 10000,\n",
    "        val_size: int = 5000,\n",
    "        pool_size: int = 45000,\n",
    "    ):\n",
    "        \"\"\"Split all training datatset into train, validate, pool sets and load them accordingly.\n",
    "\n",
    "        Attributes:\n",
    "            all_training_set: MNIST training dataset,\n",
    "            test_set: MNIST test dataset\n",
    "            training_size: Training data size,\n",
    "            val_size: Validation data size,\n",
    "            pool_size: Pool set size\n",
    "        \"\"\"\n",
    "        train_set, val_set, pool_set = random_split(\n",
    "            self.mnist_train, [train_size, val_size, pool_size]\n",
    "        )\n",
    "        train_loader = DataLoader(\n",
    "            dataset=train_set, batch_size=train_size, shuffle=True\n",
    "        )\n",
    "        val_loader = DataLoader(dataset=val_set, batch_size=val_size, shuffle=True)\n",
    "        pool_loader = DataLoader(dataset=pool_set, batch_size=pool_size, shuffle=True)\n",
    "        test_loader = DataLoader(\n",
    "            dataset=self.mnist_test, batch_size=10000, shuffle=True\n",
    "        )\n",
    "        X_train_All, y_train_All = next(iter(train_loader))\n",
    "        X_val, y_val = next(iter(val_loader))\n",
    "        X_pool, y_pool = next(iter(pool_loader))\n",
    "        X_test, y_test = next(iter(test_loader))\n",
    "        return X_train_All, y_train_All, X_val, y_val, X_pool, y_pool, X_test, y_test\n",
    "\n",
    "    def preprocess_training_data(self):\n",
    "        \"\"\"Setup a random but balanced initial training set of 20 data points\n",
    "\n",
    "        Attributes:\n",
    "            X_train_All: X input of training set,\n",
    "            y_train_All: y input of training set\n",
    "        \"\"\"\n",
    "        initial_idx = np.array([], dtype=np.int)\n",
    "        for i in range(10):\n",
    "            idx = np.random.choice(\n",
    "                np.where(self.y_train_All == i)[0], size=2, replace=False\n",
    "            )\n",
    "            initial_idx = np.concatenate((initial_idx, idx))\n",
    "        X_init = self.X_train_All[initial_idx]\n",
    "        y_init = self.y_train_All[initial_idx]\n",
    "        print(f\"Initial training data points: {X_init.shape[0]}\")\n",
    "        print(f\"Data distribution for each class: {np.bincount(y_init)}\")\n",
    "        return X_init, y_init\n",
    "\n",
    "    def load_all(self):\n",
    "        \"\"\"Load all data\"\"\"\n",
    "        return (\n",
    "            self.tensor_to_np(self.X_init),\n",
    "            self.tensor_to_np(self.y_init),\n",
    "            self.tensor_to_np(self.X_val),\n",
    "            self.tensor_to_np(self.y_val),\n",
    "            self.tensor_to_np(self.X_pool),\n",
    "            self.tensor_to_np(self.y_pool),\n",
    "            self.tensor_to_np(self.X_test),\n",
    "            self.tensor_to_np(self.y_test),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquisition Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform(model, X_pool: np.ndarray, n_query: int = 10):\n",
    "    \"\"\"Baseline acquisition a(x) = unif() with unif() a function\n",
    "    returning a draw from a uniform distribution over the interval [0,1].\n",
    "    Using this acquisition function is equivalent to choosing a point\n",
    "    uniformly at random from the pool.\n",
    "\n",
    "    Attributes:\n",
    "        X_pool: Pool set to select uncertainty,\n",
    "        n_query: Number of points that randomly select from pool set\n",
    "    \"\"\"\n",
    "    query_idx = np.random.choice(range(len(X_pool)), size=n_query, replace=False)\n",
    "    return query_idx, X_pool[query_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_entropy(model, X_pool: np.ndarray, n_query: int = 10, T: int = 100):\n",
    "    \"\"\"Choose pool points that maximise the predictive entropy.\n",
    "    Using Shannon entropy function.\n",
    "\n",
    "    Attributes:\n",
    "        model: Model that is ready to measure uncertainty after training,\n",
    "        X_pool: Pool set to select uncertainty,\n",
    "        n_query: Number of points that maximise max_entropy a(x) from pool set,\n",
    "        T: Number of MC dropout iterations aka training iterations\n",
    "    \"\"\"\n",
    "    acquisition, random_subset = shannon_entropy_function(model, X_pool, T)\n",
    "    idx = (-acquisition).argsort()[:n_query]\n",
    "    query_idx = random_subset[idx]\n",
    "    return query_idx, X_pool[query_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BALD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bald(model, X_pool: np.ndarray, n_query: int = 10, T: int = 100):\n",
    "    \"\"\"Choose pool points that are expected to maximise the information\n",
    "    gained about the model parameters, i.e. maximise the mutal information\n",
    "    between predictions and model posterior. Given\n",
    "    I[y,w|x,D_train] = H[y|x,D_train] - E_{p(w|D_train)}[H[y|x,w]]\n",
    "    with w the model parameters (H[y|x,w] is the entropy of y given w).\n",
    "    Points that maximise this acquisition function are points on which the\n",
    "    model is uncertain on average but there exist model parameters that produce\n",
    "    disagreeing predictions with high certainty. This is equivalent to points\n",
    "    with high variance in th einput to the softmax layer\n",
    "\n",
    "    Attributes:\n",
    "        model: Model that is ready to measure uncertainty after training,\n",
    "        X_pool: Pool set to select uncertainty,\n",
    "        n_query: Number of points that maximise bald a(x) from pool set,\n",
    "        T: Number of MC dropout iterations aka training iterations\n",
    "    \"\"\"\n",
    "    H, E_H, random_subset = shannon_entropy_function(model, X_pool, T, E_H=True)\n",
    "    acquisition = H - E_H\n",
    "    idx = (-acquisition).argsort()[:n_query]\n",
    "    query_idx = random_subset[idx]\n",
    "    return query_idx, X_pool[query_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Var Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_ratios(model, X_pool: np.ndarray, n_query: int = 10, T: int = 100):\n",
    "    \"\"\"Like Max Entropy but Variational Ratios measures lack of confidence.\n",
    "    Given: variational_ratio[x] := 1 - max_{y} p(y|x,D_{train})\n",
    "\n",
    "    Attributes:\n",
    "        model: Model that is ready to measure uncertainty after training,\n",
    "        X_pool: Pool set to select uncertainty,\n",
    "        n_query: Number of points that maximise var_ratios a(x) from pool set,\n",
    "        T: Number of MC dropout iterations aka training iterations\n",
    "    \"\"\"\n",
    "    outputs, random_subset = predictions_from_pool(model, X_pool, T)\n",
    "    preds = np.argmax(outputs, axis=2)\n",
    "    _, count = stats.mode(preds, axis=0)\n",
    "    acquisition = (1 - count / preds.shape[1]).reshape((-1,))\n",
    "    idx = (-acquisition).argsort()[:n_query]\n",
    "    query_idx = random_subset[idx]\n",
    "    return query_idx, X_pool[query_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_from_pool(model, X_pool: np.ndarray, T: int = 100):\n",
    "    \"\"\"Run random_subset prediction on model and return the output\n",
    "\n",
    "    Attributes:\n",
    "        X_pool: Pool set to select uncertainty,\n",
    "        T: Number of MC dropout iterations aka training iterations,\n",
    "    \"\"\"\n",
    "    random_subset = np.random.choice(range(len(X_pool)), size=2000, replace=False)\n",
    "    with torch.no_grad():\n",
    "        outputs = np.stack(\n",
    "            [\n",
    "                torch.softmax(\n",
    "                    model.estimator.forward(X_pool[random_subset], training=True),\n",
    "                    dim=-1,\n",
    "                )\n",
    "                .cpu()\n",
    "                .numpy()\n",
    "                for t in range(T)\n",
    "            ]\n",
    "        )\n",
    "    return outputs, random_subset\n",
    "\n",
    "def shannon_entropy_function(\n",
    "    model, X_pool: np.ndarray, T: int = 100, E_H: bool = False\n",
    "):\n",
    "    \"\"\"H[y|x,D_train] := - sum_{c} p(y=c|x,D_train)log p(y=c|x,D_train)\n",
    "\n",
    "    Attributes:\n",
    "        model: Model that is ready to measure uncertainty after training,\n",
    "        X_pool: Pool set to select uncertainty,\n",
    "        T: Number of MC dropout iterations aka training iterations,\n",
    "        E_H: If True, compute H and EH for BALD (default: False)\n",
    "    \"\"\"\n",
    "    outputs, random_subset = predictions_from_pool(model, X_pool, T)\n",
    "    pc = outputs.mean(axis=0)\n",
    "    H = (-pc * np.log(pc + 1e-10)).sum(axis=-1)\n",
    "    if E_H:\n",
    "        E = -np.mean(np.sum(outputs * np.log(outputs + 1e-10), axis=-1), axis=0)\n",
    "        return H, E, random_subset\n",
    "    return H, random_subset\n",
    "\n",
    "def load_CNN_model(lr, batch_size, epochs, device):\n",
    "    \"\"\"Load new model each time for different acqusition function\n",
    "    each experiments\"\"\"\n",
    "    model = ConvNN().to(device)\n",
    "    cnn_classifier = NeuralNetClassifier(\n",
    "        module=model,\n",
    "        lr=lr,\n",
    "        batch_size=batch_size,\n",
    "        max_epochs=epochs,\n",
    "        criterion=nn.CrossEntropyLoss,\n",
    "        optimizer=torch.optim.Adam,\n",
    "        train_split=None,\n",
    "        verbose=0,\n",
    "        device=device,\n",
    "    )\n",
    "    return cnn_classifier\n",
    "\n",
    "def select_acq_function(acq_func: int = 0) -> list:\n",
    "    \"\"\"Choose types of acqusition function\n",
    "\n",
    "    Attributes:\n",
    "        acq_func: 0-all(unif, max_entropy, bald), 1-unif, 2-maxentropy, 3-bald, \\\n",
    "                  4-var_ratios\n",
    "    \"\"\"\n",
    "    acq_func_dict = {\n",
    "        0: [uniform, max_entropy, bald, var_ratios],\n",
    "        1: [uniform],\n",
    "        2: [max_entropy],\n",
    "        3: [bald],\n",
    "        4: [var_ratios],\n",
    "    }\n",
    "    return acq_func_dict[acq_func]\n",
    "\n",
    "def active_learning_procedure(\n",
    "    query_strategy,\n",
    "    X_val: np.ndarray,\n",
    "    y_val: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    X_pool: np.ndarray,\n",
    "    y_pool: np.ndarray,\n",
    "    X_init: np.ndarray,\n",
    "    y_init: np.ndarray,\n",
    "    estimator,\n",
    "    T: int = 100,\n",
    "    n_query: int = 10,\n",
    ") -> list:\n",
    "    \"\"\"Active Learning Procedure\n",
    "\n",
    "    Attributes:\n",
    "        query_strategy: Choose between Uniform(baseline), max_entropy, bald,\n",
    "        X_val, y_val: Validation dataset,\n",
    "        X_test, y_test: Test dataset,\n",
    "        X_pool, y_pool: Query pool set,\n",
    "        X_init, y_init: Initial training set data points,\n",
    "        estimator: Neural Network architecture, e.g. CNN,\n",
    "        T: Number of MC dropout iterations (repeat acqusition process T times),\n",
    "        n_query: Number of points to query from X_pool\n",
    "    \"\"\"\n",
    "    learner = ActiveLearner(\n",
    "        estimator=estimator,\n",
    "        X_training=X_init,\n",
    "        y_training=y_init,\n",
    "        query_strategy=query_strategy,\n",
    "    )\n",
    "    perf_hist = [learner.score(X_test, y_test)]\n",
    "    for index in range(T):\n",
    "        query_idx, query_instance = learner.query(X_pool, n_query)\n",
    "        learner.teach(X_pool[query_idx], y_pool[query_idx], only_new=True)\n",
    "        X_pool = np.delete(X_pool, query_idx, axis=0)\n",
    "        y_pool = np.delete(y_pool, query_idx, axis=0)\n",
    "        model_accuracy_val = learner.score(X_val, y_val)\n",
    "        if (index + 1) % 5 == 0:\n",
    "            print(f\"Val Accuracy after query {index+1}: {model_accuracy_val:0.4f}\")\n",
    "        perf_hist.append(model_accuracy_val)\n",
    "    model_accuracy_test = learner.score(X_test, y_test)\n",
    "    print(f\"********** Test Accuracy per experiment: {model_accuracy_test} **********\")\n",
    "    return perf_hist, model_accuracy_test\n",
    "\n",
    "def plot_results(data: dict):\n",
    "    for key in data.keys():\n",
    "        plt.plot(data[key], label=key)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def train_active_learning(lr, batch_size, epochs, acq_funcs, experiments, dropout_iter, query, device, datasets):\n",
    "    \"\"\"Start training process\"\"\"\n",
    "    acq_functions = select_acq_function(acq_funcs)\n",
    "    results = dict()\n",
    "    for i, acq_func in enumerate(acq_functions):\n",
    "        avg_hist = []\n",
    "        test_scores = []\n",
    "        acq_func_name = str(acq_func).split(\" \")[1]\n",
    "        print(f\"\\n---------- Start {acq_func_name} training! ----------\")\n",
    "        for e in range(experiments):\n",
    "            estimator = load_CNN_model(lr, batch_size, epochs, device)\n",
    "            print(\n",
    "                f\"********** Experiment Iterations: {e+1}/{experiments} **********\"\n",
    "            )\n",
    "            training_hist, test_score = active_learning_procedure(\n",
    "                acq_func,\n",
    "                datasets[\"X_val\"],\n",
    "                datasets[\"y_val\"],\n",
    "                datasets[\"X_test\"],\n",
    "                datasets[\"y_test\"],\n",
    "                datasets[\"X_pool\"],\n",
    "                datasets[\"y_pool\"],\n",
    "                datasets[\"X_init\"],\n",
    "                datasets[\"y_init\"],\n",
    "                estimator,\n",
    "                dropout_iter,\n",
    "                query,\n",
    "            )\n",
    "            avg_hist.append(training_hist)\n",
    "            test_scores.append(test_score)\n",
    "        avg_hist = np.average(np.array(avg_hist), axis=0)\n",
    "        avg_test = sum(test_scores) / len(test_scores)\n",
    "        print(f\"Average Test score for {acq_func_name}: {avg_test}\")\n",
    "        results[acq_func_name] = avg_hist.tolist()\n",
    "    print(\"--------------- Done Training! ---------------\")\n",
    "    plot_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 50\n",
    "lr = 1e-3\n",
    "seed = 369\n",
    "experiments = 3\n",
    "dropout_iter = 100\n",
    "query = 10\n",
    "acq_funcs = 0 # 0-all, 1-uniform, 2-max_entropy, 3-bald, 4-var_ratios\n",
    "\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check device for training (cpu/ gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = dict()\n",
    "DataLoader = LoadData()\n",
    "(\n",
    "    datasets[\"X_init\"],\n",
    "    datasets[\"y_init\"],\n",
    "    datasets[\"X_val\"],\n",
    "    datasets[\"y_val\"],\n",
    "    datasets[\"X_pool\"],\n",
    "    datasets[\"y_pool\"],\n",
    "    datasets[\"X_test\"],\n",
    "    datasets[\"y_test\"],\n",
    ") = DataLoader.load_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train_active_learning(lr, \n",
    "                                batch_size, \n",
    "                                epochs, \n",
    "                                acq_funcs, \n",
    "                                experiments, \n",
    "                                dropout_iter, \n",
    "                                query, \n",
    "                                device, \n",
    "                                datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
